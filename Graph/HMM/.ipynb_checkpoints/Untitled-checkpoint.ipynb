{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# TP HMM et Viterbi"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## I Création des paramètres\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1. Récupération des corpus"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Corpus d'apprentissage"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "840659"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_data = []\n",
    "train_sentences = []\n",
    "train_target = []\n",
    "sentence = []\n",
    "train_corpus=[]\n",
    "with open(\"M1_graph/train.ester1.cut.bio\",'r') as ftrain:\n",
    "    lignes  = ftrain.readlines()\n",
    "    for line in lignes:\n",
    "        \n",
    "        if line=='\\n':\n",
    "            train_sentences.append(sentence)\n",
    "            sentence = []\n",
    "        else:\n",
    "            train_data.append(line.split(\" \")[0])\n",
    "            train_target.append(line.split(\" \")[1].replace('\\n',''))\n",
    "            train_corpus.append((line.split(\" \")[0],line.split(\" \")[1].replace('\\n','')))\n",
    "            sentence.append((line.split(\" \")[0],line.split(\" \")[1].replace('\\n','')))\n",
    "    train_sentences.append(sentence)\n",
    "    train_data = np.array(train_data)\n",
    "    train_target = np.array(train_target)\n",
    "len(train_corpus)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Corpus de Développement"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "dev_data = []\n",
    "dev_target = []\n",
    "dev_sentences=[]\n",
    "sentence = []\n",
    "with open(\"M1_graph/dev.ester1.cut.bio\",'r') as fdev:\n",
    "    lignes  = fdev.readlines()\n",
    "    for line in lignes:\n",
    "        if len(line)<2:\n",
    "            dev_sentences.append(sentence)\n",
    "            sentence=[]\n",
    "        else:\n",
    "            dev_data.append(line.split(\" \")[0])\n",
    "            dev_target.append(line.split(\" \")[1].replace('\\n',''))\n",
    "    \n",
    "            sentence.append((line.split(\" \")[0],line.split(\" \")[1].replace('\\n','')))\n",
    "    dev_sentences.append(sentence)\n",
    "    dev_data = np.array(dev_data)\n",
    "    dev_target = np.array(dev_target)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Corpus de Test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_data = []\n",
    "test_target = []\n",
    "test_sentences = []\n",
    "sentence = []\n",
    "with open(\"M1_graph/test.ester1.cut.bio\",'r') as ftest:\n",
    "    lignes  = ftest.readlines()    \n",
    "    for line in lignes:\n",
    "        if len(line)<2:\n",
    "            test_sentences.append(sentence)\n",
    "            sentence=[]\n",
    "        else:\n",
    "            test_data.append(line.split(\" \")[0])\n",
    "            test_target.append(line.split(\" \")[1].replace('\\n',''))\n",
    "            sentence.append((line.split(\" \")[0],line.split(\" \")[1].replace('\\n','')))\n",
    "            \n",
    "    test_sentences.append(sentence)\n",
    "    test_data = np.array(test_data)\n",
    "    test_target = np.array(test_target)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Récupération du nombre d'étiquette et de mots différents.\n",
    "\n",
    "Pour aider au traitement des mots, on affecte un nombre à chacun"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "list_word = {}\n",
    "for index,word in enumerate(list(set(list(train_data)+list(test_data)+list(dev_data)))):\n",
    "    list_word[word]=index\n",
    "nb_etiq = len(set(train_target))\n",
    "nb_word = len(set(list_word))\n",
    "word_numbers = {}\n",
    "tag_numbers = {}\n",
    "corpus = train_sentences\n",
    "test_corpus = test_sentences\n",
    "num_corpus = []\n",
    "num_test_corpus = []\n",
    "for sent in corpus:\n",
    "    num_sent = []\n",
    "    for word, tag in sent:\n",
    "        wi = word_numbers.setdefault(word.lower(), len(word_numbers))\n",
    "        ti = tag_numbers.setdefault(tag, len(tag_numbers))\n",
    "        num_sent.append((wi, ti))\n",
    "    num_corpus.append(num_sent)\n",
    "for sent in test_corpus:\n",
    "    num_sent = []\n",
    "    for word, tag in sent:\n",
    "        wi = word_numbers.setdefault(word.lower(), len(word_numbers))\n",
    "        ti = tag_numbers.setdefault(tag, len(tag_numbers))\n",
    "        num_sent.append((wi, ti))\n",
    "    num_test_corpus.append(num_sent)\n",
    "word_names = [None] * len(word_numbers)\n",
    "for word, index in word_numbers.items():\n",
    "    word_names[index] = word\n",
    "tag_names = [None] * len(tag_numbers)\n",
    "for tag, index in tag_numbers.items():\n",
    "    tag_names[index] = tag"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([-0.13555203, -4.21253956, -4.50427027, -4.56890128, -4.46328956,\n",
       "       -5.5360018 , -4.94324542, -4.12066085, -4.23487715, -4.01928787,\n",
       "       -5.01821216, -5.63321173, -5.07498886, -6.97996056, -6.28050421])"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "list_etiquette = set(train_target)\n",
    "proba_ini_etiq = 0.1 * np.ones(nb_etiq)\n",
    "for sentence in num_corpus:\n",
    "    for word,tag in sentence:\n",
    "        proba_ini_etiq[tag]+=1\n",
    "\n",
    "proba_ini_etiq /= np.sum(proba_ini_etiq)\n",
    "proba_ini_etiq = np.log(proba_ini_etiq)\n",
    "proba_ini_etiq"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "15\n"
     ]
    }
   ],
   "source": [
    "transition = 0.1*np.ones((nb_etiq, nb_etiq))\n",
    "newTransition = []\n",
    "\n",
    "for sentence in num_corpus:\n",
    "    pred = None\n",
    "    for word, tag in sentence:\n",
    "        transition[pred, tag] += 1\n",
    "        pred = tag\n",
    "print(nb_etiq)\n",
    "for s in range(nb_etiq):\n",
    "    transition[s,:] /= np.sum(transition[s,:])\n",
    "    transition[s,:] = np.log(transition[s,:])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "list_proba_emission = 0.1 * np.ones((nb_etiq, nb_word))       \n",
    "for word in num_corpus:\n",
    "    for value ,tag in word:\n",
    "        list_proba_emission[tag,value]+=1\n",
    "for s in range(nb_etiq):\n",
    "    list_proba_emission[s,:] /= np.sum(list_proba_emission[s,:])\n",
    "    list_proba_emission[s,:] = np.log(list_proba_emission[s,:])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def viterbi2(params, observations):\n",
    "    pi, A, O = params\n",
    "    M = len(observations)\n",
    "    S = pi.shape[0]\n",
    "    \n",
    "    alpha = np.zeros((M, S))\n",
    "    alpha[:,:] = float('-inf')\n",
    "    backpointers = np.zeros((M, S), 'int')\n",
    "    # base case\n",
    "    alpha[0, :] = pi + O[:,observations[0]]\n",
    "    \n",
    "    # recursive case\n",
    "    for t in range(1, M):\n",
    "        for s2 in range(S):\n",
    "            for s1 in range(S):\n",
    "                score = alpha[t-1, s1] + A[s1, s2] + O[s2, observations[t]]\n",
    "                if score > alpha[t, s2]:\n",
    "                    alpha[t, s2] = score\n",
    "                    backpointers[t, s2] = s1\n",
    "    \n",
    "    # now follow backpointers to resolve the state sequence\n",
    "    ss = []\n",
    "    ss.append(np.argmax(alpha[M-1,:]))\n",
    "    for i in range(M-1, 0, -1):\n",
    "        ss.append(backpointers[i, ss[-1]])\n",
    "        \n",
    "    return list(reversed(ss)), np.max(alpha[M-1,:])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DONE\n"
     ]
    }
   ],
   "source": [
    "\n",
    "with open(\"resne.dat\",\"w\") as f:\n",
    "    for sentences in num_test_corpus:        \n",
    "        \n",
    "        predicted, score = viterbi2((proba_ini_etiq, transition, list_proba_emission), [x[0] for x in sentences])\n",
    "        \n",
    "        for tuple in zip([tag_names[x[1]] for x in sentences],[tag_names[x]for x in predicted]):\n",
    "            \n",
    "            f.write(str(tuple)+'\\n')\n",
    "        f.write(str((None,None))+'\\n')\n",
    "        \n",
    "print(\"DONE\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
