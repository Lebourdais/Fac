{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#! /usr/bin/python\n",
    "\n",
    "__author__=\"Daniel Bauer <bauer@cs.columbia.edu>\"\n",
    "__date__ =\"$Sep 29, 2011\"\n",
    "\n",
    "import sys\n",
    "\n",
    "\n",
    "\"\"\"\n",
    "Evaluate named entity tagger output by comparing it to a gold standard file.\n",
    "\n",
    "Running the script on your tagger output like this\n",
    "\n",
    "    python eval_ne_tagger.py ner_dev.key your_tagger_output.dat\n",
    "\n",
    "will generate a table of results like this:\n",
    "\n",
    "    Found 14071 NEs. Expected 5942 NEs; Correct: 3120.\n",
    "\n",
    "\t\t precision \trecall \t\tF1-Score\n",
    "    Total:\t 0.221733\t0.525076\t0.311797\n",
    "    PER:\t 0.433367\t0.231270\t0.301593\n",
    "    ORG:\t 0.475089\t0.398210\t0.433266\n",
    "    LOC:\t 0.147597\t0.869352\t0.252350\n",
    "    MISC:\t 0.492133\t0.610629\t0.545015\n",
    "\n",
    "\n",
    "The script compares the named entities found by your tagger to the annotations\n",
    "in the gold standard file. A named entity is correct if both its span (the set\n",
    "of tokens) and the type of the entity are matched.\n",
    "Precision is the number of correct named entities found by your tagger out of\n",
    "all named entities it found.\n",
    "Recall is the number of correct named entities found by your tagger out of all\n",
    "named entities marked in the gold standard.\n",
    "F1-Score is a measure of total performance of your tagger and is computed from\n",
    "precision p and recall r as F = 2 * p * r / (p+r).\n",
    "\n",
    "The \"Total\" column shows results for all named entity types.\n",
    "The remaining columns show how well your tagger performs on each named entity\n",
    "type in isolation.\n",
    "\"\"\"\n",
    "\n",
    "def corpus_iterator(corpus_file, with_logprob = False):\n",
    "    \"\"\"\n",
    "    Get an iterator object over the corpus file. The elements of the\n",
    "    iterator contain (word, ne_tag) tuples. Blank lines, indicating\n",
    "    sentence boundaries return (None, None).\n",
    "    \"\"\"\n",
    "    l = corpus_file.readline()    \n",
    "    tagfield = with_logprob and -2 or -1\n",
    "\n",
    "    try:\n",
    "        while l:\n",
    "            line = l.strip()\n",
    "            if line: # Nonempty line\n",
    "                # Extract information from line.\n",
    "                # Each line has the format\n",
    "                # word ne_tag [log_prob]\n",
    "                fields = line.split(\" \")\n",
    "                ne_tag = fields[tagfield]\n",
    "                word = \" \".join(fields[:tagfield])\n",
    "                yield word, ne_tag\n",
    "            else: # Empty line\n",
    "                yield (None, None)\n",
    "            l = corpus_file.readline()\n",
    "    except IndexError:\n",
    "        sys.stderr.write(\"Could not read line: \\n\")\n",
    "        sys.stderr.write(\"\\n%s\" % line)\n",
    "        if with_logprob:\n",
    "            sys.stderr.write(\"Did you forget to output log probabilities in the prediction file?\\n\")\n",
    "        sys.exit(1)\n",
    "\n",
    "\n",
    "class NeTypeCounts(object):\n",
    "    \"\"\"\n",
    "    Stores true/false positive/negative counts for each NE type.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self):\n",
    "        self.tp = 0\n",
    "        self.fp = 0\n",
    "        self.tn = 0\n",
    "        self.fn = 0 \n",
    "\n",
    "    def get_precision(self):\n",
    "        return self.tp / float(self.tp + self.fp)\n",
    "\n",
    "    def get_recall(self):\n",
    "        return self.tp / float(self.tp + self.fn)\n",
    "\n",
    "    def get_accuracy(self):\n",
    "        return (self.tp + self.tn) / float(self.tp + self.tn + self.fp + self.fn)\n",
    "\n",
    "\n",
    "class Evaluator(object):\n",
    "    \"\"\"\n",
    "    Stores global true/false positive/negative counts. \n",
    "    \"\"\"\n",
    "\n",
    "\n",
    "    ne_classes = [\"PER\",\"ORG\",\"LOC\",\"MISC\"]\n",
    "\n",
    "    def __init__(self):        \n",
    "        self.tp = 0\n",
    "        self.tn = 0\n",
    "        self.fp = 0        \n",
    "        self.fn = 0\n",
    "\n",
    "        # Initialize an object that counts true/false positives/negatives\n",
    "        # for each NE class\n",
    "        self.class_counts = {}\n",
    "        for c in self.ne_classes:\n",
    "            self.class_counts[c] = NeTypeCounts()\n",
    "\n",
    "    def compare(self, gold_standard, prediction):\n",
    "        \"\"\"\n",
    "        Compare the prediction against a gold standard. Both objects must be\n",
    "        generator or iterator objects that return a (word, ne_tag) tuple at a\n",
    "        time.\n",
    "        \"\"\"\n",
    "\n",
    "        # Define a couple of tags indicating the status of each stream\n",
    "        curr_pred_type = None # prediction stream was previously in a named entity\n",
    "        curr_pred_start = None # a new prediction starts at the current token\n",
    "        curr_gs_type = None   # prediction stream was previously in a named entity\n",
    "        curr_gs_start = None # a new prediction starts at the current token\n",
    "\n",
    "        total = 0\n",
    "        for gs_word, gs_tag in gold_standard: # Move through the gold standard stream\n",
    "            pred_word, pred_tag = prediction.next() # Get the corresponding item from the prediction stream\n",
    "            \n",
    "            # Make sure words in both files match up\n",
    "            if gs_word != pred_word:\n",
    "                sys.stderr.write(\"Could not align gold standard and predictions in line %i.\\n\" % (total+1))\n",
    "                sys.stderr.write(\"Gold standard: %s  Prediction file: %s\\n\" % (gs_word, pred_word))\n",
    "                sys.exit(1)        \n",
    "\n",
    "            # Split off the I and B tags\n",
    "            gs_type = gs_tag==None and \"O\" or gs_tag.split(\"-\")[-1]\n",
    "            pred_type = pred_tag==None and \"O\" or pred_tag.split(\"-\")[-1]                        \n",
    "\n",
    "            # Check if a named entity ends here in either stream.\n",
    "            # This is the case if we are currently in an entity and either\n",
    "            #   - end of sentence\n",
    "            #   - current word is marked O\n",
    "            #   - new entity starts (B - or I with different NE type)\n",
    "            pred_ends = curr_pred_type!=None and ((pred_tag==None or pred_tag[0] in \"OB\") or (curr_pred_type!=pred_type and pred_tag[0]==\"I\"))\n",
    "            gs_ends = curr_gs_type!=None and ((gs_tag==None or gs_tag[0] in \"OB\") or (curr_gs_type!=gs_type and gs_tag[0]==\"I\"))\n",
    "            \n",
    "\n",
    "            # Check if a named entity starts here in either stream.\n",
    "            # This is tha case if this is not the end of a sentence and\n",
    "            #   - This is not the end of a sentence\n",
    "            #   - New entity starts (B, I after O or at begining of sentence or\n",
    "            #       I with different NE type) \n",
    "            if pred_word!=None:\n",
    "                pred_start = (pred_tag!=None and pred_tag[0] == \"B\") or (curr_pred_type==None and pred_tag[0]==\"I\") or \\\n",
    "                    (curr_pred_type!=None and curr_pred_type!=pred_type and pred_tag.startswith(\"I\"))\n",
    "                gs_starts = (gs_tag!=None and gs_tag[0] == \"B\") or (curr_gs_type==None and gs_tag[0]==\"I\") or \\\n",
    "                    (curr_gs_type!=None and curr_gs_type!=gs_type and gs_tag.startswith(\"I\"))\n",
    "            else:\n",
    "                pred_start = False\n",
    "                gs_starts = False            \n",
    "\n",
    "            #For debugging:\n",
    "            #print pred_word, gs_tag, pred_tag, pred_ends, gs_ends, pred_start, gs_starts\n",
    "\n",
    "\n",
    "            # Now try to match up named entities that end here\n",
    "\n",
    "            if gs_ends and pred_ends: # GS and prediction contain a named entity that ends in the same place\n",
    "\n",
    "                #If both named entities start at the same place and are of the same type\n",
    "                if curr_gs_start == curr_pred_start and curr_gs_type == curr_pred_type:\n",
    "                    # Count true positives\n",
    "                    self.tp += 1\n",
    "                    self.class_counts[curr_pred_type].tp += 1\n",
    "                else: #span matches, but label doesn't match: count both a true positive and a false negative\n",
    "                    self.fp += 1\n",
    "                    self.fn += 1\n",
    "                    self.class_counts[curr_pred_type].fp += 1\n",
    "                    self.class_counts[curr_gs_type].fn += 1\n",
    "            elif gs_ends: #Didn't find the named entity in the gold standard, count false negative\n",
    "                self.fn += 1\n",
    "                self.class_counts[curr_gs_type].fn += 1\n",
    "            elif pred_ends: #Named entity in the prediction doesn't match one int he gold_standard, count false positive\n",
    "                self.fp += 1\n",
    "                self.class_counts[curr_pred_type].fp += 1\n",
    "            elif curr_pred_type==None and curr_pred_type==None: #matching O tag or end of sentence, count true negative\n",
    "                self.tn += 1\n",
    "                for c in self.ne_classes:\n",
    "                    self.class_counts[c].tn += 1\n",
    "\n",
    "            # Remember that we are no longer in a named entity\n",
    "            if gs_ends:\n",
    "                curr_gs_type = None\n",
    "            if pred_ends:\n",
    "                curr_pred_type = None\n",
    "\n",
    "            # If a named entity starts here, remember it's type and this position\n",
    "            if gs_starts:\n",
    "                curr_gs_start = total\n",
    "                curr_gs_type = gs_type\n",
    "            if pred_start:\n",
    "                curr_pred_start = total\n",
    "                curr_pred_type = pred_type\n",
    "            total += 1\n",
    "\n",
    "    def print_scores(self):\n",
    "        \"\"\"\n",
    "        Output a table with accuracy, precision, recall and F1 score. \n",
    "        \"\"\"\n",
    "\n",
    "        print \"Found %i NEs. Expected %i NEs; Correct: %i.\\n\" % (self.tp + self.fp, self.tp + self.fn, self.tp)\n",
    "\n",
    "\n",
    "        if self.tp + self.tn + self.fp + self.fn == 0: # There was nothing to do.\n",
    "            acc = 1\n",
    "        else:\n",
    "            acc = (self.tp + self.tn) / float(self.tp + self.tn + self.fp + self.fn)\n",
    "\n",
    "        if self.tp+self.fp == 0:   # Prediction didn't annotate any NEs\n",
    "            prec = 1\n",
    "            \n",
    "        else:\n",
    "            prec = self.tp / float(self.tp + self.fp)\n",
    "            \n",
    "\n",
    "        if self.tp+self.fn == 0: # Prediction marked everything as a NE of the wrong type.\n",
    "            rec = 1\n",
    "        else:\n",
    "            rec = self.tp / float(self.tp + self.fn)\n",
    "\n",
    "        print \"\\t precision \\trecall \\t\\tF1-Score\"\n",
    "        fscore = (2*prec*rec)/(prec+rec)\n",
    "        print \"Total:\\t %f\\t%f\\t%f\" % (prec, rec, fscore)\n",
    "        for c in self.ne_classes:\n",
    "            c_tp = self.class_counts[c].tp\n",
    "            c_tn = self.class_counts[c].tn\n",
    "            c_fp = self.class_counts[c].fp\n",
    "            c_fn = self.class_counts[c].fn\n",
    "            #print c\n",
    "            #print c_tp\n",
    "            #print c_tn\n",
    "            #print c_fp\n",
    "            #print c_fn\n",
    "            if (c_tp + c_tn + c_fp + c_fn) == 0:                \n",
    "                c_acc = 1\n",
    "            else:\n",
    "                c_acc = (c_tp + c_tn) / float(c_tp + c_tn + c_fp + c_fn)\n",
    "            \n",
    "            if (c_tp + c_fn) == 0:\n",
    "                sys.stderr.write(\"Warning: no instances for entity type %s in gold standard.\\n\" % c)\n",
    "                c_rec = 1\n",
    "            else:\n",
    "                c_rec = c_tp / float(c_tp + c_fn)\n",
    "            if (c_tp + c_fp) == 0:\n",
    "                sys.stderr.write(\"Warning: prediction file does not contain any instances of entity type %s.\\n\" % c)\n",
    "                c_prec =1\n",
    "            else:\n",
    "                c_prec = c_tp / float(c_tp + c_fp)\n",
    "\n",
    "            if c_prec + c_rec == 0:\n",
    "                fscore = 0\n",
    "            else:    \n",
    "                fscore = (2*c_prec * c_rec)/(c_prec + c_rec)\n",
    "            print \"%s:\\t %f\\t%f\\t%f\" % (c, c_prec, c_rec, fscore)\n",
    "\n",
    "\n",
    "def usage():\n",
    "    sys.stderr.write(\"\"\"\n",
    "    Usage: python eval_ne_tagger.py [key_file] [prediction_file]\n",
    "        Evaluate the NE-tagger output in prediction_file against\n",
    "        the gold standard in key_file. Output accuracy, precision,\n",
    "        recall and F1-Score for each NE tag type.\\n\"\"\")\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "\n",
    "    if len(sys.argv)!=3:\n",
    "        usage()\n",
    "        sys.exit(1)\n",
    "    gs_iterator = corpus_iterator(file(sys.argv[1]))\n",
    "    pred_iterator = corpus_iterator(file(sys.argv[2]), with_logprob = True)\n",
    "    evaluator = Evaluator()\n",
    "    evaluator.compare(gs_iterator, pred_iterator)\n",
    "    evaluator.print_scores()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
